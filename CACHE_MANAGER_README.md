# Cache Manager - Complete Documentation

## ðŸ“‹ Overview

The **CacheManager** is a high-level cache management system that provides application-level caching with automatic serialization, compression, and prefetch coordination. It sits on top of the raw cache backend and adds intelligence for handling Python objects.

## ðŸŽ¯ Key Features

### âœ¨ Core Features
- âœ… **Automatic Serialization** - Handles Python object conversion (pickle or JSON)
- âœ… **Intelligent Compression** - Optional zlib compression for large values
- âœ… **Backend Abstraction** - Works with InMemory or Redis backends
- âœ… **Prefetch Coordination** - Queue and prioritize prefetch requests
- âœ… **RL Integration** - Eviction methods for RL-driven cache optimization
- âœ… **Comprehensive Metrics** - Track all operations and performance

### ðŸŽ¨ Design Principles
- **Simple API** - High-level operations on Python objects
- **Pluggable Backends** - Easy to switch between memory and Redis
- **Performance Optimized** - Compression only when beneficial
- **Production Ready** - Error handling, logging, thread safety

## ðŸ“¦ Components

### 1. CacheManagerConfig

Configuration dataclass for the cache manager.

```python
@dataclass
class CacheManagerConfig:
    backend_type: str = 'memory'              # 'redis' or 'memory'
    backend_config: Optional[Any] = None      # Backend-specific config
    default_ttl: int = 300                    # Default TTL in seconds
    max_entry_size: int = 1024 * 1024         # 1MB max per entry
    compression_enabled: bool = True          # Enable compression
    compression_threshold: int = 1024         # Compress if > 1KB
    compression_level: int = 6                # zlib level (1-9)
    serialization_format: str = 'pickle'      # 'pickle' or 'json'
```

**Configuration Options:**

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `backend_type` | str | 'memory' | Backend to use ('redis' or 'memory') |
| `backend_config` | Any | None | Configuration for the backend |
| `default_ttl` | int | 300 | Default time-to-live in seconds |
| `max_entry_size` | int | 1MB | Maximum size for a single entry |
| `compression_enabled` | bool | True | Whether to compress values |
| `compression_threshold` | int | 1KB | Minimum size before compressing |
| `compression_level` | int | 6 | zlib compression level (1-9) |
| `serialization_format` | str | 'pickle' | Serialization format |

### 2. CacheManager

Main cache manager class.

#### Initialization and Lifecycle

```python
# Create manager
config = CacheManagerConfig(backend_type='memory')
manager = CacheManager(config)

# Start the manager
manager.start()

# Check if running
if manager.is_running:
    print("Manager is running")

# Stop the manager
manager.stop()
```

#### High-Level Cache Operations

**get(key, default=None)**
```python
# Retrieve and deserialize
user = manager.get('/api/users/123')
if user is None:
    user = {'id': 123, 'name': 'John'}

# With default value
settings = manager.get('/api/settings', default={'theme': 'light'})
```

**set(key, value, ttl=None, metadata=None)**
```python
# Store Python object
user = {'id': 123, 'name': 'John', 'email': 'john@example.com'}
manager.set('/api/users/123', user, ttl=300)

# With metadata
manager.set(
    '/api/products/42',
    {'id': 42, 'name': 'Widget'},
    ttl=600,
    metadata={'endpoint': '/api/products', 'user_type': 'premium'}
)
```

**delete(key)**
```python
# Remove from cache
success = manager.delete('/api/users/123')
```

**get_or_set(key, factory, ttl=None)**
```python
# Get from cache or generate
def fetch_user():
    return database.get_user(123)

user = manager.get_or_set('/api/users/123', fetch_user, ttl=300)
# Factory is only called on cache miss
```

#### Prefetch Integration

**prefetch(endpoint, params=None, priority=0.5)**
```python
# Queue single prefetch
manager.prefetch('/api/users/123/orders', priority=0.8)
manager.prefetch(
    '/api/products',
    params={'category': 'electronics'},
    priority=0.6
)
```

**prefetch_many(items)**
```python
# Queue multiple prefetches
items = [
    ('/api/users/1', None, 0.7),
    ('/api/users/2', None, 0.6),
    ('/api/products/42', {'details': 'full'}, 0.9),
]
manager.prefetch_many(items)
```

**get_prefetch_queue()**
```python
# Get current queue for debugging
queue = manager.get_prefetch_queue()
for item in queue:
    print(f"{item['endpoint']} - priority {item['priority']}")
```

#### Eviction Control (for RL)

**evict_lru(count=10)**
```python
# Manually evict LRU entries
evicted = manager.evict_lru(count=10)
print(f"Evicted {evicted} entries")
```

**evict_low_probability(predictions, count=10)**
```python
# Evict based on Markov predictions
predictions = {
    '/api/users/1': 0.1,   # Low probability
    '/api/users/2': 0.8,   # High probability
    '/api/users/3': 0.05,  # Very low probability
}

evicted = manager.evict_low_probability(predictions, count=2)
# Evicts entries with lowest access probability
```

#### Metrics

**get_metrics()**
```python
metrics = manager.get_metrics()

print(f"Hit rate: {metrics['hit_rate']:.2%}")
print(f"Compression ratio: {metrics['compression_ratio']:.2%}")
print(f"Prefetch queue size: {metrics['prefetch_queue_size']}")
```

**Metrics Dictionary:**
```python
{
    # Backend stats
    'hits': 100,
    'misses': 20,
    'sets': 120,
    'deletes': 10,
    'hit_rate': 0.833,
    'current_entries': 110,
    'current_size_bytes': 1024000,
    
    # Manager stats
    'serialization_time_ms': 5.2,
    'deserialization_time_ms': 3.1,
    'compression_time_ms': 2.0,
    'decompression_time_ms': 1.5,
    'compression_ratio': 0.35,
    'compression_count': 50,
    'prefetch_requests': 30,
    'prefetch_hits': 25,
    'prefetch_hit_rate': 0.833,
    'cache_operations': 250,
    'serialization_errors': 0,
    'compression_errors': 0,
    'prefetch_queue_size': 5,
}
```

### 3. Helper Functions

**generate_cache_key(endpoint, params=None)**
```python
from src.cache.cache_manager import generate_cache_key

# Simple key
key = generate_cache_key('/api/users/123')
# Result: '/api/users/123'

# With parameters (sorted for determinism)
key = generate_cache_key('/api/users', {'id': 123, 'filter': 'active'})
# Result: '/api/users|filter=active&id=123'

# Long keys are hashed
long_params = {f'param{i}': f'value{i}' for i in range(100)}
key = generate_cache_key('/api/endpoint', long_params)
# Result: '/api/endpoint|hash:abc123...'
```

## ðŸš€ Usage Examples

### Basic Usage

```python
from src.cache.cache_manager import CacheManager, CacheManagerConfig

# Create and start manager
config = CacheManagerConfig(
    backend_type='memory',
    compression_enabled=True,
    default_ttl=300
)

manager = CacheManager(config)
manager.start()

try:
    # Cache user data
    user = {'id': 123, 'name': 'John', 'email': 'john@example.com'}
    manager.set('/api/users/123', user, ttl=60)
    
    # Retrieve user
    retrieved = manager.get('/api/users/123')
    print(f"User: {retrieved}")
    
finally:
    manager.stop()
```

### With Redis Backend

```python
from src.cache.cache_manager import CacheManager, CacheManagerConfig
from src.cache.redis_backend import RedisConfig

# Configure Redis
redis_config = RedisConfig(
    host='localhost',
    port=6379,
    key_prefix='myapp:'
)

# Create manager with Redis
config = CacheManagerConfig(
    backend_type='redis',
    backend_config=redis_config,
    default_ttl=600
)

manager = CacheManager(config)
manager.start()
```

### Get or Set Pattern

```python
def expensive_computation():
    # Simulate expensive operation
    time.sleep(1)
    return {'result': 42, 'computed_at': time.time()}

# First call - computes and caches
result = manager.get_or_set(
    '/api/expensive_operation',
    expensive_computation,
    ttl=300
)

# Second call - uses cache (instant)
result = manager.get_or_set(
    '/api/expensive_operation',
    expensive_computation,
    ttl=300
)
```

### Prefetch Coordination

```python
# Predict next likely requests
predictions = markov_model.predict(current_state)

# Queue prefetches by probability
for endpoint, probability in predictions.items():
    if probability > 0.5:
        manager.prefetch(endpoint, priority=probability)

# Check queue
queue = manager.get_prefetch_queue()
print(f"Queued {len(queue)} prefetches")
```

### RL-Driven Eviction

```python
# Get predictions from Markov model
all_keys = ['/api/users/1', '/api/users/2', '/api/products/42']
predictions = {}

for key in all_keys:
    predictions[key] = markov_model.predict_access_probability(key)

# Evict low-probability entries
evicted = manager.evict_low_probability(predictions, count=10)
print(f"Evicted {evicted} unlikely-to-be-accessed entries")
```

## ðŸŽ¨ Architecture

### Data Flow

```
Application Request
        â†“
  CacheManager.get()
        â†“
  [Check Backend] â”€â†’ HIT â”€â†’ Decompress â†’ Deserialize â†’ Return
        â†“
       MISS â”€â†’ Return None/Default
```

```
Application Store
        â†“
  CacheManager.set()
        â†“
  Serialize â†’ Compress (if large) â†’ Backend.set()
```

### Compression Strategy

1. Check if compression enabled
2. If data < threshold, skip compression
3. Compress with zlib
4. Only use if compressed < original
5. Prepend marker byte (0x00=uncompressed, 0x01=compressed)

### Serialization Formats

**Pickle (default)**
- Pros: Handles all Python objects, faster
- Cons: Python-specific, security concerns with untrusted data

**JSON**
- Pros: Language-agnostic, safe, human-readable
- Cons: Limited types, slower

## ðŸ“Š Performance

### Benchmarks (In-Memory Backend)

| Operation | Time | Throughput |
|-----------|------|------------|
| get() | <1ms | >10,000 ops/sec |
| set() | <1ms | >10,000 ops/sec |
| get_or_set() | <1ms | >10,000 ops/sec |
| Serialization | <0.1ms | - |
| Compression | 0.5-2ms | - |

### Memory Usage

- Overhead: ~100-200 bytes per entry
- Compression: 20-40% reduction for text
- Serialization: Varies by format and data

## ðŸ”§ Configuration Best Practices

### Development
```python
config = CacheManagerConfig(
    backend_type='memory',
    compression_enabled=False,  # Skip compression for speed
    default_ttl=60,             # Short TTL
    serialization_format='pickle'
)
```

### Production
```python
config = CacheManagerConfig(
    backend_type='redis',
    compression_enabled=True,
    compression_threshold=1024,
    default_ttl=300,
    max_entry_size=10 * 1024 * 1024,  # 10MB
    serialization_format='pickle'
)
```

### High Performance
```python
config = CacheManagerConfig(
    backend_type='redis',
    compression_enabled=True,
    compression_threshold=10240,     # Higher threshold
    compression_level=3,             # Faster compression
    default_ttl=600,
    serialization_format='pickle'    # Faster than JSON
)
```

## ðŸ› Error Handling

### Serialization Errors
```python
try:
    manager.set('key', unpicklable_object)
except Exception as e:
    logger.error(f"Serialization failed: {e}")
    # Falls back gracefully, increments error counter
```

### Backend Connection Errors
```python
if not manager.start():
    logger.error("Failed to start cache manager")
    # Handle fallback to no-cache mode
```

### Compression Errors
```python
# Manager handles compression errors internally
# Falls back to uncompressed data
# Increments compression_errors metric
```

## ðŸ“ Logging

The manager logs all significant operations:

```python
import logging
logging.basicConfig(level=logging.DEBUG)

# Logs include:
# - Manager start/stop
# - Cache hits/misses
# - Prefetch requests
# - Evictions
# - Errors
```

## ðŸŽ¯ Integration with RL System

### State Representation
```python
# Cache state for RL agent
metrics = manager.get_metrics()
state = {
    'hit_rate': metrics['hit_rate'],
    'utilization': metrics['current_size_bytes'] / max_size,
    'prefetch_queue_size': metrics['prefetch_queue_size'],
}
```

### Action Execution
```python
# RL agent decides action
action = rl_agent.select_action(state)

if action == 'evict':
    predictions = markov_model.predict_all()
    manager.evict_low_probability(predictions, count=10)

elif action == 'prefetch':
    items = markov_model.predict_next_requests()
    manager.prefetch_many(items)
```

### Reward Calculation
```python
metrics_before = manager.get_metrics()
# Execute action
metrics_after = manager.get_metrics()

# Calculate reward
hit_rate_improvement = (
    metrics_after['hit_rate'] - metrics_before['hit_rate']
)
reward = hit_rate_improvement * 100
```

## ðŸš€ Advanced Usage

### Custom Backend

```python
from src.cache.backend import CacheBackend

class MyCustomBackend(CacheBackend):
    # Implement required methods
    pass

# Use with manager (requires modifying _create_backend)
```

### Metrics Monitoring

```python
import time

# Periodic metrics collection
while True:
    metrics = manager.get_metrics()
    
    # Send to monitoring system
    monitoring.record('cache_hit_rate', metrics['hit_rate'])
    monitoring.record('cache_size', metrics['current_entries'])
    
    time.sleep(60)
```

### Prefetch Worker

```python
def prefetch_worker(manager):
    """Background worker to process prefetch queue."""
    while True:
        queue = manager.get_prefetch_queue()
        
        for item in queue[:10]:  # Process top 10
            endpoint = item['endpoint']
            params = item['params']
            
            # Fetch and cache
            data = api.fetch(endpoint, params)
            key = generate_cache_key(endpoint, params)
            manager.set(key, data)
        
        time.sleep(1)

# Run in background thread
import threading
thread = threading.Thread(target=prefetch_worker, args=(manager,))
thread.daemon = True
thread.start()
```

## ðŸ“š API Reference

See the implementation for complete API documentation with type hints and docstrings.

## âœ… Validation

Run the validation script:

```bash
python validate_cache_manager.py
```

Expected output:
- âœ… All tests pass
- âœ… Hit rate > 75%
- âœ… Compression works
- âœ… Prefetch queue functional
- âœ… All data types supported

---

**Status**: âœ… Complete and Production-Ready

The CacheManager provides a robust, high-level caching solution with intelligent features for production use.

