"""
Example: Building a TransitionMatrix from SequenceBuilder output.

This demonstrates how to integrate TransitionMatrix with the existing
preprocessing pipeline to create a Markov chain model from API logs.
"""

import json
from pathlib import Path
from src.markov import TransitionMatrix


def build_from_sequences(sequences_file: str, output_file: str = "transition_matrix.json", smoothing: float = 0.001):
    """
    Build a TransitionMatrix from sequences generated by SequenceBuilder.

    Args:
        sequences_file: Path to sequences.json from SequenceBuilder
        output_file: Where to save the transition matrix
        smoothing: Laplace smoothing parameter

    Returns:
        TransitionMatrix: The built transition matrix
    """
    print(f"Loading sequences from {sequences_file}...")

    # Load sequences
    with open(sequences_file, 'r') as f:
        data = json.load(f)

    # Handle both list and dict formats
    if isinstance(data, list):
        sequences = data
    else:
        sequences = data.get('sequences', [])

    print(f"  Found {len(sequences)} sequences")

    # Build transition matrix
    print(f"\nBuilding transition matrix (smoothing={smoothing})...")
    tm = TransitionMatrix(smoothing=smoothing)

    total_transitions = 0
    for sequence in sequences:
        # Each sequence is a list of API endpoints
        for i in range(len(sequence) - 1):
            from_endpoint = sequence[i]
            to_endpoint = sequence[i + 1]
            tm.increment(from_endpoint, to_endpoint)
            total_transitions += 1

    print(f"  Processed {total_transitions} transitions")

    # Get statistics
    stats = tm.get_statistics()
    print(f"\nMatrix Statistics:")
    print(f"  Unique endpoints: {stats['num_states']}")
    print(f"  Unique transitions: {stats['num_transitions']}")
    print(f"  Sparsity: {stats['sparsity']:.2%}")
    print(f"  Avg transitions per endpoint: {stats['avg_transitions_per_state']:.2f}")

    print(f"\nTop 10 Most Common Transitions:")
    for i, trans in enumerate(stats['most_common_transitions'][:10], 1):
        print(f"  {i:2d}. {trans['from']:20s} → {trans['to']:20s}: {trans['count']:4d}")

    # Save matrix
    print(f"\nSaving transition matrix to {output_file}...")
    tm.save(output_file)

    file_size = Path(output_file).stat().st_size
    print(f"  File size: {file_size:,} bytes")

    return tm


def analyze_cache_opportunities(tm: TransitionMatrix, top_k: int = 5):
    """
    Analyze which endpoints would benefit most from prefetching.

    Args:
        tm: Transition matrix
        top_k: Number of top candidates to show per endpoint
    """
    print(f"\n{'='*70}")
    print("Cache Prefetching Analysis")
    print('='*70)

    # Get all source states (endpoints that transition to others)
    source_states = list(tm.transitions.keys())

    # Calculate predictability score for each endpoint
    # Score = sum of top-k probabilities (how predictable are next states?)
    predictability = []
    for state in source_states:
        top = tm.get_top_k(state, k=top_k)
        if top:
            score = sum(prob for _, prob in top)
            avg_transitions = tm.total_from[state]
            predictability.append((state, score, avg_transitions, top))

    # Sort by predictability
    predictability.sort(key=lambda x: x[1], reverse=True)

    print(f"\nEndpoints with HIGHEST cache prefetch potential:")
    print(f"(More predictable = better prefetch candidates)\n")

    for i, (state, score, count, top) in enumerate(predictability[:10], 1):
        print(f"{i:2d}. {state:25s} (predictability: {score:.1%}, {count} transitions)")
        for endpoint, prob in top[:3]:
            print(f"      → {endpoint:20s}: {prob:.2%}")

    print(f"\nEndpoints with LOWEST cache prefetch potential:")
    print(f"(Less predictable = don't waste cache space)\n")

    for i, (state, score, count, top) in enumerate(predictability[-5:], 1):
        print(f"{i:2d}. {state:25s} (predictability: {score:.1%}, {count} transitions)")
        for endpoint, prob in top[:3]:
            print(f"      → {endpoint:20s}: {prob:.2%}")


def generate_prefetch_rules(tm: TransitionMatrix, threshold: float = 0.15, output_file: str = "prefetch_rules.json"):
    """
    Generate cache prefetching rules based on transition probabilities.

    Args:
        tm: Transition matrix
        threshold: Minimum probability to include in prefetch rules
        output_file: Where to save rules
    """
    print(f"\n{'='*70}")
    print(f"Generating Prefetch Rules (threshold={threshold:.1%})")
    print('='*70)

    rules = {}
    total_rules = 0

    for source_state in tm.transitions.keys():
        row = tm.get_row(source_state)
        candidates = [(endpoint, prob) for endpoint, prob in row.items() if prob >= threshold]

        if candidates:
            # Sort by probability
            candidates.sort(key=lambda x: x[1], reverse=True)
            rules[source_state] = [
                {"endpoint": endpoint, "probability": prob}
                for endpoint, prob in candidates
            ]
            total_rules += len(candidates)

    print(f"\nGenerated {total_rules} prefetch rules for {len(rules)} endpoints")

    # Show examples
    print(f"\nExample Rules:")
    for i, (source, targets) in enumerate(list(rules.items())[:5], 1):
        print(f"\n{i}. When user accesses '{source}', prefetch:")
        for target in targets:
            print(f"     • {target['endpoint']:20s} (probability: {target['probability']:.2%})")

    # Save rules
    print(f"\nSaving rules to {output_file}...")
    with open(output_file, 'w') as f:
        json.dump(rules, f, indent=2)

    print(f"  ✓ Saved {total_rules} prefetch rules")

    return rules


def compare_time_periods(sequences_file1: str, sequences_file2: str, label1: str = "Period 1", label2: str = "Period 2"):
    """
    Compare transition patterns between two time periods.

    Args:
        sequences_file1: First time period sequences
        sequences_file2: Second time period sequences
        label1: Label for first period
        label2: Label for second period
    """
    print(f"\n{'='*70}")
    print(f"Comparing Transition Patterns")
    print('='*70)

    # Build matrices for both periods
    print(f"\nBuilding matrix for {label1}...")
    tm1 = TransitionMatrix(smoothing=0.001)
    with open(sequences_file1, 'r') as f:
        data1 = json.load(f)
        sequences1 = data1 if isinstance(data1, list) else data1.get('sequences', [])

    for seq in sequences1:
        for i in range(len(seq) - 1):
            tm1.increment(seq[i], seq[i + 1])

    print(f"Building matrix for {label2}...")
    tm2 = TransitionMatrix(smoothing=0.001)
    with open(sequences_file2, 'r') as f:
        data2 = json.load(f)
        sequences2 = data2 if isinstance(data2, list) else data2.get('sequences', [])

    for seq in sequences2:
        for i in range(len(seq) - 1):
            tm2.increment(seq[i], seq[i + 1])

    # Compare statistics
    stats1 = tm1.get_statistics()
    stats2 = tm2.get_statistics()

    print(f"\n{label1:20s} | {label2:20s}")
    print("-" * 70)
    print(f"States:      {stats1['num_states']:8d}      | {stats2['num_states']:8d}")
    print(f"Transitions: {stats1['num_transitions']:8d}      | {stats2['num_transitions']:8d}")
    print(f"Sparsity:    {stats1['sparsity']:7.2%}      | {stats2['sparsity']:7.2%}")

    # Find changes in top transitions
    top1 = {(t['from'], t['to']): t['count'] for t in stats1['most_common_transitions'][:10]}
    top2 = {(t['from'], t['to']): t['count'] for t in stats2['most_common_transitions'][:10]}

    print(f"\nNew popular transitions in {label2}:")
    for (from_ep, to_ep) in top2:
        if (from_ep, to_ep) not in top1:
            print(f"  • {from_ep:20s} → {to_ep:20s}")

    print(f"\nDecreased popularity from {label1}:")
    for (from_ep, to_ep) in top1:
        if (from_ep, to_ep) not in top2:
            print(f"  • {from_ep:20s} → {to_ep:20s}")


def main():
    """Example integration workflow."""
    print("\n" + "╔" + "═" * 68 + "╗")
    print("║" + " " * 10 + "TransitionMatrix + SequenceBuilder Integration" + " " * 11 + "║")
    print("╚" + "═" * 68 + "╝")

    # Check if example data exists
    sequences_file = "data/final_test/sequences.json"

    if not Path(sequences_file).exists():
        print(f"\n⚠ Example data not found: {sequences_file}")
        print("\nTo generate example data, run:")
        print("  python demo_sequence_builder.py")
        print("\nOr use the preprocessing pipeline:")
        print("  python scripts/preprocess.py --synthetic --output data/test")
        return

    try:
        # 1. Build transition matrix from sequences
        tm = build_from_sequences(
            sequences_file=sequences_file,
            output_file="example_transition_matrix.json",
            smoothing=0.001
        )

        # 2. Analyze cache opportunities
        analyze_cache_opportunities(tm, top_k=5)

        # 3. Generate prefetch rules
        rules = generate_prefetch_rules(tm, threshold=0.15, output_file="example_prefetch_rules.json")

        # 4. Example: Query specific endpoint
        print(f"\n{'='*70}")
        print("Example Queries")
        print('='*70)

        # Get a sample endpoint
        sample_endpoint = list(tm.transitions.keys())[0] if tm.transitions else None

        if sample_endpoint:
            print(f"\nWhat happens after user accesses '{sample_endpoint}'?")
            top = tm.get_top_k(sample_endpoint, k=3)

            print(f"\nTop 3 most likely next endpoints:")
            for i, (endpoint, prob) in enumerate(top, 1):
                print(f"  {i}. {endpoint:25s}: {prob:.2%}")

            # Cache decision
            print(f"\nCache Prefetch Decision:")
            threshold = 0.15
            for endpoint, prob in top:
                if prob >= threshold:
                    print(f"  ✓ PREFETCH {endpoint:20s} ({prob:.1%} > {threshold:.1%})")
                else:
                    print(f"  ✗ Skip     {endpoint:20s} ({prob:.1%} < {threshold:.1%})")

        print(f"\n{'='*70}")
        print("Integration Complete!")
        print('='*70)

        print("\nGenerated Files:")
        print("  • example_transition_matrix.json - Full transition matrix")
        print("  • example_prefetch_rules.json    - Prefetch rules for cache")

        print("\nNext Steps:")
        print("  1. Load matrix in your cache system")
        print("  2. Use prefetch rules for API request prediction")
        print("  3. Integrate with RL agent for adaptive caching")
        print("  4. Monitor and update matrix with new data")

        print("\nExample Cache Integration:")
        print("""
    from src.markov import TransitionMatrix
    
    # Load trained matrix
    tm = TransitionMatrix.load("example_transition_matrix.json")
    
    # On each API request
    def on_request(endpoint):
        # Get likely next endpoints
        next_endpoints = tm.get_top_k(endpoint, k=3)
        
        # Prefetch if probability > threshold
        for next_ep, prob in next_endpoints:
            if prob > 0.15:
                cache.prefetch(next_ep)
        """)

    except Exception as e:
        print(f"\n✗ Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

